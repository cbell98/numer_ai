{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTc0OJEJ77Sb"
      },
      "source": [
        "## Loading required libraries üìî and dataset üóÑÔ∏èüîΩ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dukzbOx5YPL2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab011861-f6da-464e-caad-a3857d405b22"
      },
      "source": [
        "# installing required libraries\n",
        "# numerapi, for facilitating data download and predictions uploading\n",
        "\n",
        "!pip install numerapi\n",
        "!pip install xgboost\n",
        "!pip install utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numerapi\n",
            "  Downloading numerapi-2.13.3-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.9/dist-packages (from numerapi) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from numerapi) (2.8.2)\n",
            "Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.9/dist-packages (from numerapi) (4.65.0)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from numerapi) (1.4.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from numerapi) (2.27.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from numerapi) (8.1.3)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.0->numerapi) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil->numerapi) (1.16.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->numerapi) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->numerapi) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->numerapi) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->numerapi) (1.26.15)\n",
            "Installing collected packages: numerapi\n",
            "Successfully installed numerapi-2.13.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.9/dist-packages (1.7.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from xgboost) (1.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from xgboost) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting utils\n",
            "  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3qA9k0VZ4Hj"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "from tqdm import tqdm\n",
        "from xgboost import XGBRegressor\n",
        "import gc\n",
        "import json\n",
        "from pathlib import Path\n",
        "from scipy.stats import skew\n",
        "\n",
        "from numerapi import NumerAPI\n",
        "import utils\n",
        "# from utils import (\n",
        "#     save_model,\n",
        "#     load_model,\n",
        "#     neutralize,\n",
        "#     get_biggest_change_features,\n",
        "#     validation_metrics,\n",
        "#     ERA_COL,\n",
        "#     DATA_TYPE_COL,\n",
        "#     TARGET_COL,\n",
        "#     EXAMPLE_PREDS_COL\n",
        "# )"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrzPVfR6egjj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "a9d62697-a2d3-4506-9285-b41f0ae60a7b"
      },
      "source": [
        "# download all the things\n",
        "\n",
        "napi = NumerAPI()\n",
        "\n",
        "current_round = napi.get_current_round()\n",
        "\n",
        "# Tournament data changes every week so we specify the round in their name. Training\n",
        "# and validation data only change periodically, so no need to download them every time.\n",
        "print('Downloading dataset files...')\n",
        "\n",
        "Path(\"./v4\").mkdir(parents=False, exist_ok=True)\n",
        "napi.download_dataset(\"v4/train.parquet\")\n",
        "napi.download_dataset(\"v4/validation.parquet\")\n",
        "napi.download_dataset(\"v4/live.parquet\", f\"v4/live_{current_round}.parquet\")\n",
        "napi.download_dataset(\"v4/validation_example_preds.parquet\")\n",
        "napi.download_dataset(\"v4/features.json\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:numerapi.base_api:Current round not open for submissions\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-bbd143d8aa61>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNumerAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcurrent_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_current_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Tournament data changes every week so we specify the round in their name. Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numerapi/base_api.py\u001b[0m in \u001b[0;36mget_current_round\u001b[0;34m(self, tournament)\u001b[0m\n\u001b[1;32m    286\u001b[0m         '''\n\u001b[1;32m    287\u001b[0m         \u001b[0marguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'tournament'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtournament\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rounds'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numerapi/base_api.py\u001b[0m in \u001b[0;36mraw_query\u001b[0;34m(self, query, variables, authorization, retries, delay, backoff)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;31m# fail!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Current round not open for submissions"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHdD3oxv8D5d"
      },
      "source": [
        "## Build Feature Set\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ERA_COL = \"era\"\n",
        "TARGET_COL = \"target_nomi_v4_20\"\n",
        "DATA_TYPE_COL = \"data_type\"\n",
        "EXAMPLE_PREDS_COL = \"example_preds\"\n",
        "\n",
        "MODEL_FOLDER = \"models\""
      ],
      "metadata": {
        "id": "Q4A2oCX0ArgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHATa1jEq01J"
      },
      "source": [
        "print('Reading minimal training data')\n",
        "# read the feature metadata and get a feature set (or all the features)\n",
        "with open(\"v4/features.json\", \"r\") as f:\n",
        "    feature_metadata = json.load(f)\n",
        "# features = list(feature_metadata[\"feature_stats\"].keys()) # get all the features\n",
        "features = feature_metadata[\"feature_sets\"][\"small\"] # get the small feature set\n",
        "# features = feature_metadata[\"feature_sets\"][\"medium\"] # get the medium feature set\n",
        "# read in just those features along with era and target columns\n",
        "read_columns = features + [ERA_COL, DATA_TYPE_COL, TARGET_COL]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxqczTVq8PR4"
      },
      "source": [
        "## Loading and exploring dataset into memory üñ•Ô∏è"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgodzImqq7z3"
      },
      "source": [
        "# note: sometimes when trying to read the downloaded data you get an error about invalid magic parquet bytes...\n",
        "# if so, delete the file and rerun the napi.download_dataset to fix the corrupted file\n",
        "training_data = pd.read_parquet('v4/train.parquet',\n",
        "                                columns=read_columns)\n",
        "validation_data = pd.read_parquet('v4/validation.parquet',\n",
        "                                  columns=read_columns)\n",
        "live_data = pd.read_parquet(f'v4/live_{current_round}.parquet',\n",
        "                                  columns=read_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKGYq1Ce7MHX"
      },
      "source": [
        "# pare down the number of eras to every 4th era\n",
        "every_4th_era = training_data[ERA_COL].unique()[::4]\n",
        "training_data = training_data[training_data[ERA_COL].isin(every_4th_era)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyVjLKTy3tMx"
      },
      "source": [
        "training_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFZAE25OyYNa"
      },
      "source": [
        "live_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the per era correlation of each feature vs the target\n",
        "all_feature_corrs = training_data.groupby(ERA_COL).apply(\n",
        "    lambda era: era[features].corrwith(era[TARGET_COL])\n",
        ")"
      ],
      "metadata": {
        "id": "jQYjQ_RnB6p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function\n",
        "\n",
        "def get_biggest_change_features(corrs, n):\n",
        "    all_eras = corrs.index.sort_values()\n",
        "    h1_eras = all_eras[: len(all_eras) // 2]\n",
        "    h2_eras = all_eras[len(all_eras) // 2 :]\n",
        "\n",
        "    h1_corr_means = corrs.loc[h1_eras, :].mean()\n",
        "    h2_corr_means = corrs.loc[h2_eras, :].mean()\n",
        "\n",
        "    corr_diffs = h2_corr_means - h1_corr_means\n",
        "    worst_n = corr_diffs.abs().sort_values(ascending=False).head(n).index.tolist()\n",
        "    return worst_n"
      ],
      "metadata": {
        "id": "_KHfYaC4CGiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the riskiest features by comparing their correlation vs\n",
        "# the target in each half of training data; we'll use these later\n",
        "riskiest_features = get_biggest_change_features(all_feature_corrs, 50)"
      ],
      "metadata": {
        "id": "hrK01S7mB_Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"garbage collection\" (gc) gets rid of unused data and frees up memory\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "sM8_H6AxB_TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwdqS642BP-u"
      },
      "source": [
        "## Training our model ü§ñ‚öôÔ∏è\n",
        "\n",
        "This is where most of tweaking will happen. You can add more model in your pipeline simply by changing your model and data pipeline suited for that architecture."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Functions\n",
        "\n",
        "def save_prediction(df, name):\n",
        "    try:\n",
        "        Path(PREDICTION_FILES_FOLDER).mkdir(exist_ok=True, parents=True)\n",
        "    except Exception as ex:\n",
        "        pass\n",
        "    df.to_csv(f\"{PREDICTION_FILES_FOLDER}/{name}.csv\", index=True)\n",
        "\n",
        "\n",
        "def save_model(model, name):\n",
        "    try:\n",
        "        Path(MODEL_FOLDER).mkdir(exist_ok=True, parents=True)\n",
        "    except Exception as ex:\n",
        "        pass\n",
        "    pd.to_pickle(model, f\"{MODEL_FOLDER}/{name}.pkl\")\n",
        "\n",
        "\n",
        "def load_model(name):\n",
        "    path = Path(f\"{MODEL_FOLDER}/{name}.pkl\")\n",
        "    if path.is_file():\n",
        "        model = pd.read_pickle(f\"{MODEL_FOLDER}/{name}.pkl\")\n",
        "    else:\n",
        "        model = False\n",
        "    return model"
      ],
      "metadata": {
        "id": "e9EcQGomCtXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGCEWaizehA9"
      },
      "source": [
        "model_name = f\"model_target\"\n",
        "print(f\"Checking for existing model '{model_name}'\")\n",
        "model = load_model(model_name)\n",
        "if not model:\n",
        "    print(f\"model not found, creating new one\")\n",
        "    params = {\"n_estimators\": 2000,\n",
        "              \"learning_rate\": 0.01,\n",
        "              \"max_depth\": 5,\n",
        "              \"colsample_bytree\": 0.1}\n",
        "\n",
        "    model = XGBRegressor(**params)\n",
        "\n",
        "    # train on all of train and save the model so we don't have to train next time\n",
        "    model.fit(training_data.filter(like='feature_', axis='columns'),\n",
        "              training_data[TARGET_COL])\n",
        "    print(f\"saving new model: {model_name}\")\n",
        "    save_model(model, model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "uAbQoIyBEHJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nans_per_col = live_data[live_data[\"data_type\"] == \"live\"][features].isna().sum()\n",
        "\n",
        "# check for nans and fill nans\n",
        "if nans_per_col.any():\n",
        "    total_rows = len(live_data[live_data[\"data_type\"] == \"live\"])\n",
        "    print(f\"Number of nans per column this week: {nans_per_col[nans_per_col > 0]}\")\n",
        "    print(f\"out of {total_rows} total rows\")\n",
        "    print(f\"filling nans with 0.5\")\n",
        "    live_data.loc[:, features] = live_data.loc[:, features].fillna(0.5)\n",
        "\n",
        "else:\n",
        "    print(\"No nans in the features this week!\")"
      ],
      "metadata": {
        "id": "MU7KUYxlEP2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# double check the feature that the model expects vs what is available to prevent our\n",
        "# pipeline from failing if Numerai adds more data and we don't have time to retrain!\n",
        "model_expected_features = model.get_booster().get_score(importance_type='gain')\n",
        "if set(model_expected_features) != set(features):\n",
        "    print(f\"New features are available! Might want to retrain model {model_name}.\")\n",
        "validation_data.loc[:, f\"preds_{model_name}\"] = model.predict(\n",
        "    validation_data.loc[:, model_expected_features])\n",
        "live_data.loc[:, f\"preds_{model_name}\"] = model.predict(\n",
        "    live_data.loc[:, model_expected_features])"
      ],
      "metadata": {
        "id": "uS1oA1tmEQAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "w1Ie0N2BEi6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neutralize"
      ],
      "metadata": {
        "id": "S2bWN_itEvF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function\n",
        "\n",
        "def neutralize(\n",
        "    df, columns, neutralizers=None, proportion=1.0, normalize=True, era_col=\"era\", verbose=False\n",
        "):\n",
        "    if neutralizers is None:\n",
        "        neutralizers = []\n",
        "    unique_eras = df[era_col].unique()\n",
        "    computed = []\n",
        "    if verbose:\n",
        "        iterator = tqdm(unique_eras)\n",
        "    else:\n",
        "        iterator = unique_eras\n",
        "    for u in iterator:\n",
        "        df_era = df[df[era_col] == u]\n",
        "        scores = df_era[columns].values\n",
        "        if normalize:\n",
        "            scores2 = []\n",
        "            for x in scores.T:\n",
        "                x = (scipy.stats.rankdata(x, method=\"ordinal\") - 0.5) / len(x)\n",
        "                x = scipy.stats.norm.ppf(x)\n",
        "                scores2.append(x)\n",
        "            scores = np.array(scores2).T\n",
        "        exposures = df_era[neutralizers].values\n",
        "\n",
        "        scores -= proportion * exposures.dot(\n",
        "            np.linalg.pinv(exposures.astype(np.float32), rcond=1e-6).dot(\n",
        "                scores.astype(np.float32)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        scores /= scores.std(ddof=0)\n",
        "\n",
        "        computed.append(scores)\n",
        "\n",
        "    return pd.DataFrame(np.concatenate(computed), columns=columns, index=df.index)"
      ],
      "metadata": {
        "id": "WmbasVe2E4Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# neutralize our predictions to the riskiest features\n",
        "validation_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(\n",
        "    df=validation_data,\n",
        "    columns=[f\"preds_{model_name}\"],\n",
        "    neutralizers=riskiest_features,\n",
        "    proportion=1.0,\n",
        "    normalize=True,\n",
        "    era_col=ERA_COL\n",
        ")\n",
        "\n",
        "live_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(\n",
        "    df=live_data,\n",
        "    columns=[f\"preds_{model_name}\"],\n",
        "    neutralizers=riskiest_features,\n",
        "    proportion=1.0,\n",
        "    normalize=True,\n",
        "    era_col=ERA_COL\n",
        ")"
      ],
      "metadata": {
        "id": "Kd5kRxHHE4WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1ZceR9FAaPa"
      },
      "source": [
        "## Predictions. Evaluation. ‚û°Ô∏è"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAAMLN13eg9z"
      },
      "source": [
        "model_to_submit = f\"preds_{model_name}_neutral_riskiest_50\"\n",
        "\n",
        "# rename best model to \"prediction\" and rank from 0 to 1 to meet upload requirements\n",
        "validation_data[\"prediction\"] = validation_data[model_to_submit].rank(pct=True)\n",
        "live_data[\"prediction\"] = live_data[model_to_submit].rank(pct=True)\n",
        "validation_data[\"prediction\"].to_csv(f\"validation_predictions_{current_round}.csv\")\n",
        "live_data[\"prediction\"].to_csv(f\"live_predictions_{current_round}.csv\")\n",
        "\n",
        "validation_preds = pd.read_parquet('v4/validation_example_preds.parquet')\n",
        "validation_data[EXAMPLE_PREDS_COL] = validation_preds[\"prediction\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Functions\n",
        "\n",
        "def neutralize_series(series, by, proportion=1.0):\n",
        "    scores = series.values.reshape(-1, 1)\n",
        "    exposures = by.values.reshape(-1, 1)\n",
        "\n",
        "    # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n",
        "    exposures = np.hstack(\n",
        "        (exposures, np.array([np.mean(series)] * len(exposures)).reshape(-1, 1))\n",
        "    )\n",
        "\n",
        "    correction = proportion * (\n",
        "        exposures.dot(np.linalg.lstsq(exposures, scores, rcond=None)[0])\n",
        "    )\n",
        "    corrected_scores = scores - correction\n",
        "    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n",
        "    return neutralized\n",
        "\n",
        "\n",
        "def unif(df):\n",
        "    x = (df.rank(method=\"first\") - 0.5) / len(df)\n",
        "    return pd.Series(x, index=df.index)\n",
        "\n",
        "\n",
        "def get_feature_neutral_mean(\n",
        "    df, prediction_col, target_col, features_for_neutralization=None\n",
        "):\n",
        "    if features_for_neutralization is None:\n",
        "        features_for_neutralization = [c for c in df.columns if c.startswith(\"feature\")]\n",
        "    df.loc[:, \"neutral_sub\"] = neutralize(\n",
        "        df, [prediction_col], features_for_neutralization\n",
        "    )[prediction_col]\n",
        "    scores = (\n",
        "        df.groupby(\"era\")\n",
        "        .apply(lambda x: (unif(x[\"neutral_sub\"]).corr(x[target_col])))\n",
        "        .mean()\n",
        "    )\n",
        "    return np.mean(scores)\n",
        "\n",
        "\n",
        "def get_feature_neutral_mean_tb_era(\n",
        "    df, prediction_col, target_col, tb, features_for_neutralization=None\n",
        "):\n",
        "    if features_for_neutralization is None:\n",
        "        features_for_neutralization = [c for c in df.columns if c.startswith(\"feature\")]\n",
        "    temp_df = df.reset_index(\n",
        "        drop=True\n",
        "    ).copy()  # Reset index due to use of argsort later\n",
        "    temp_df.loc[:, \"neutral_sub\"] = neutralize(\n",
        "        temp_df, [prediction_col], features_for_neutralization\n",
        "    )[prediction_col]\n",
        "    temp_df_argsort = temp_df.loc[:, \"neutral_sub\"].argsort()\n",
        "    temp_df_tb_idx = pd.concat([temp_df_argsort.iloc[:tb], temp_df_argsort.iloc[-tb:]])\n",
        "    temp_df_tb = temp_df.loc[temp_df_tb_idx]\n",
        "    tb_fnc = unif(temp_df_tb[\"neutral_sub\"]).corr(temp_df_tb[target_col])\n",
        "    return tb_fnc\n",
        "\n",
        "\n",
        "def fast_score_by_date(df, columns, target, tb=None, era_col=\"era\"):\n",
        "    unique_eras = df[era_col].unique()\n",
        "    computed = []\n",
        "    for u in unique_eras:\n",
        "        df_era = df[df[era_col] == u]\n",
        "        era_pred = np.float64(df_era[columns].values.T)\n",
        "        era_target = np.float64(df_era[target].values.T)\n",
        "\n",
        "        if tb is None:\n",
        "            ccs = np.corrcoef(era_target, era_pred)[0, 1:]\n",
        "        else:\n",
        "            tbidx = np.argsort(era_pred, axis=1)\n",
        "            tbidx = np.concatenate([tbidx[:, :tb], tbidx[:, -tb:]], axis=1)\n",
        "            ccs = [\n",
        "                np.corrcoef(era_target[tmpidx], tmppred[tmpidx])[0, 1]\n",
        "                for tmpidx, tmppred in zip(tbidx, era_pred)\n",
        "            ]\n",
        "            ccs = np.array(ccs)\n",
        "\n",
        "        computed.append(ccs)\n",
        "\n",
        "    return pd.DataFrame(np.array(computed), columns=columns, index=df[era_col].unique())\n",
        "\n",
        "\n",
        "def exposure_dissimilarity_per_era(df, prediction_col, example_col, feature_cols=None):\n",
        "    if feature_cols is None:\n",
        "        feature_cols = [c for c in df.columns if c.startswith(\"feature\")]\n",
        "    u = df.loc[:, feature_cols].corrwith(df[prediction_col])\n",
        "    e = df.loc[:, feature_cols].corrwith(df[example_col])\n",
        "    return 1 - (np.dot(u, e) / np.dot(e, e))\n",
        "\n",
        "\n",
        "def validation_metrics(\n",
        "    validation_data,\n",
        "    pred_cols,\n",
        "    example_col,\n",
        "    fast_mode=False,\n",
        "    target_col=TARGET_COL,\n",
        "    features_for_neutralization=None,\n",
        "):\n",
        "    validation_stats = pd.DataFrame()\n",
        "    feature_cols = [c for c in validation_data if c.startswith(\"feature_\")]\n",
        "    for pred_col in pred_cols:\n",
        "        # Check the per-era correlations on the validation set (out of sample)\n",
        "        validation_correlations = validation_data.groupby(ERA_COL).apply(\n",
        "            lambda d: unif(d[pred_col]).corr(d[target_col])\n",
        "        )\n",
        "\n",
        "        mean = validation_correlations.mean()\n",
        "        std = validation_correlations.std(ddof=0)\n",
        "        sharpe = mean / std\n",
        "\n",
        "        validation_stats.loc[\"mean\", pred_col] = mean\n",
        "        validation_stats.loc[\"std\", pred_col] = std\n",
        "        validation_stats.loc[\"sharpe\", pred_col] = sharpe\n",
        "\n",
        "        rolling_max = (\n",
        "            (validation_correlations + 1)\n",
        "            .cumprod()\n",
        "            .rolling(window=9000, min_periods=1)  # arbitrarily large\n",
        "            .max()\n",
        "        )\n",
        "        daily_value = (validation_correlations + 1).cumprod()\n",
        "        max_drawdown = -((rolling_max - daily_value) / rolling_max).max()\n",
        "        validation_stats.loc[\"max_drawdown\", pred_col] = max_drawdown\n",
        "\n",
        "        payout_scores = validation_correlations.clip(-0.25, 0.25)\n",
        "        payout_daily_value = (payout_scores + 1).cumprod()\n",
        "\n",
        "        apy = (\n",
        "            ((payout_daily_value.dropna().iloc[-1]) ** (1 / len(payout_scores)))\n",
        "            ** 49  # 52 weeks of compounding minus 3 for stake compounding lag\n",
        "            - 1\n",
        "        ) * 100\n",
        "\n",
        "        validation_stats.loc[\"apy\", pred_col] = apy\n",
        "\n",
        "        if not fast_mode:\n",
        "            # Check the feature exposure of your validation predictions\n",
        "            max_per_era = validation_data.groupby(ERA_COL).apply(\n",
        "                lambda d: d[feature_cols].corrwith(d[pred_col]).abs().max()\n",
        "            )\n",
        "            max_feature_exposure = max_per_era.mean()\n",
        "            validation_stats.loc[\n",
        "                \"max_feature_exposure\", pred_col\n",
        "            ] = max_feature_exposure\n",
        "\n",
        "            # Check feature neutral mean\n",
        "            feature_neutral_mean = get_feature_neutral_mean(\n",
        "                validation_data, pred_col, target_col, features_for_neutralization\n",
        "            )\n",
        "            validation_stats.loc[\n",
        "                \"feature_neutral_mean\", pred_col\n",
        "            ] = feature_neutral_mean\n",
        "\n",
        "            # Check TB200 feature neutral mean\n",
        "            tb200_feature_neutral_mean_era = validation_data.groupby(ERA_COL).apply(\n",
        "                lambda df: get_feature_neutral_mean_tb_era(\n",
        "                    df, pred_col, target_col, 200, features_for_neutralization\n",
        "                )\n",
        "            )\n",
        "            validation_stats.loc[\n",
        "                \"tb200_feature_neutral_mean\", pred_col\n",
        "            ] = tb200_feature_neutral_mean_era.mean()\n",
        "\n",
        "            # Check top and bottom 200 metrics (TB200)\n",
        "            tb200_validation_correlations = fast_score_by_date(\n",
        "                validation_data, [pred_col], target_col, tb=200, era_col=ERA_COL\n",
        "            )\n",
        "\n",
        "            tb200_mean = tb200_validation_correlations.mean()[pred_col]\n",
        "            tb200_std = tb200_validation_correlations.std(ddof=0)[pred_col]\n",
        "            tb200_sharpe = tb200_mean / tb200_std\n",
        "\n",
        "            validation_stats.loc[\"tb200_mean\", pred_col] = tb200_mean\n",
        "            validation_stats.loc[\"tb200_std\", pred_col] = tb200_std\n",
        "            validation_stats.loc[\"tb200_sharpe\", pred_col] = tb200_sharpe\n",
        "\n",
        "        # MMC over validation\n",
        "        mmc_scores = []\n",
        "        corr_scores = []\n",
        "        for _, x in validation_data.groupby(ERA_COL):\n",
        "            series = neutralize_series(unif(x[pred_col]), (x[example_col]))\n",
        "            mmc_scores.append(np.cov(series, x[target_col])[0, 1] / (0.29**2))\n",
        "            corr_scores.append(unif(x[pred_col]).corr(x[target_col]))\n",
        "\n",
        "        val_mmc_mean = np.mean(mmc_scores)\n",
        "        val_mmc_std = np.std(mmc_scores)\n",
        "        corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n",
        "        corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n",
        "\n",
        "        validation_stats.loc[\"mmc_mean\", pred_col] = val_mmc_mean\n",
        "        validation_stats.loc[\"corr_plus_mmc_sharpe\", pred_col] = corr_plus_mmc_sharpe\n",
        "\n",
        "        # Check correlation with example predictions\n",
        "        per_era_corrs = validation_data.groupby(ERA_COL).apply(\n",
        "            lambda d: unif(d[pred_col]).corr(unif(d[example_col]))\n",
        "        )\n",
        "        corr_with_example_preds = per_era_corrs.mean()\n",
        "        validation_stats.loc[\n",
        "            \"corr_with_example_preds\", pred_col\n",
        "        ] = corr_with_example_preds\n",
        "\n",
        "        # Check exposure dissimilarity per era\n",
        "        tdf = validation_data.groupby(ERA_COL).apply(\n",
        "            lambda df: exposure_dissimilarity_per_era(\n",
        "                df, pred_col, example_col, feature_cols\n",
        "            )\n",
        "        )\n",
        "        validation_stats.loc[\"exposure_dissimilarity_mean\", pred_col] = tdf.mean()\n",
        "\n",
        "    # .transpose so that stats are columns and the model_name is the row\n",
        "    return validation_stats.transpose()"
      ],
      "metadata": {
        "id": "jiwIKfs5HTBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2r269Xaeo9_"
      },
      "source": [
        "# get some stats about each of our models to compare...\n",
        "# fast_mode=True so that we skip some of the stats that are slower to calculate\n",
        "validation_stats = validation_metrics(validation_data, [model_to_submit, f\"preds_{model_name}\"], example_col=EXAMPLE_PREDS_COL, fast_mode=True, target_col=TARGET_COL)\n",
        "print(validation_stats[[\"mean\", \"sharpe\"]].to_markdown())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWjIbe_emZtd"
      },
      "source": [
        "print(f'''\n",
        "Done! Next steps:\n",
        "    1. Go to numer.ai/tournament (make sure you have an account)\n",
        "    2. Submit validation_predictions_{current_round}.csv to the diagnostics tool\n",
        "    3. Submit tournament_predictions_{current_round}.csv to the \"Upload Predictions\" button\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}